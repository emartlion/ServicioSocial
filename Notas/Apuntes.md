# Transformers

#transformers #general #teoria


## Introducci√≥n: ¬øQu√© son los Transformers?

Los Transformers son una clase de modelos de aprendizaje profundo (*deep learning*) caracterizado por ciertos rasgos en su arquitectura.

La arquitectura original de Transformer es una instancia espec√≠fica de los modelos encoder-decorder populares al rededor del a√±o 2015. Hasta ese punto, la [[Atencion]] era s√≥lo uno de los mecanismos utilizados por estos modelos, que estaban basados principalmente en LSTM (memoria de Largo Corto Plazo) y otras RNN (redes neuronales recurrentes). La idea clave del art√≠culo original de los Transformers es, como su t√≠tulo lo dice, que la atenci√≥n puede ser usado como el √∫nico mecanismo para derivar dependencias entre la entrada y la salida.

La entrada de los Transformers es una secuencia de tokens. La salida del **encoder** es una representaci√≥n de dimensi√≥n fija para cada uno de los tokens junto con una incrustaci√≥n[^1] por separado de la secuencia como un todo. El **decoder** toma la salida del **encoder** como entrada y escupe (xd) una secuencia de tokens como su salida.

Desde la publicaci√≥n del art√≠culo original, modelos populares como BERT y GPT han usado s√≥lo las ideas del encoder o decoder de la arquitectura original.



## Arquitectura Codificador/Decodificador

Una arquitectura gen√©rica de codificador/decodificador est√° compuesta de dos modelos. El codificador toma la entrada y la codifica en un vector de longitud fija. En el caso de la arquitectura original de los Transformers, tanto el codificador como el decodificador ten√≠an 6 capas id√©nticas. En cada una de esas 6 capas el Encoder ten√≠a dos subcapas:

- Una capa de atenci√≥n de m√∫ltiples cabezas
- Una red neuronal de propagaci√≥n directa simple.

La capa de auto-atenci√≥n computa la representaci√≥n de salida de cada uno de sus tokens de entrada basado en todos los tokens de entrada.

![Arquitectura original del Transformer](media/ArquitecturaOriginalTransformer.png)

La capa de atenci√≥n de m√∫ltiples cabezas en el **decodificador** es ligeramente diferente que aquella del codificador. En ella se enmascaran todos los tokens a la derecha del token cuya representaci√≥n esta siendo computada para asegurarse de que el decodificador solo pueda atender a los tokens que vienen antes del token que est√° intentando predecir. El decodificador tambi√©n tambi√©n tiene a√±adido una tercera subcapa, que se trata de otra capa de atenci√≥n de m√∫ltiples cabezas sobre todas las salidas del codificador.



## Atenci√≥n

Es claro de la descripci√≥n de arriba que los √∫nicos elementos *ex√≥ticos* de la arquitectura del modelo son las capas de atenci√≥n de m√∫ltiples cabeza, pero es justo ah√≠ donde se encuentra el poder del modelo. Las m√∫ltiples cabezas de atenci√≥n permiten paralelizar el procesamiento de datos, lo que permite aprovechar el poder computacional de TPUs, GPUs, entre otros, para hacer m√°s eficiente el entrenamiento y procesamiento de datos.

Cada token en la entrada de la capa de atenci√≥n se convierte en tres vectores asociados a √©l:

‚ùì Query,
üóùÔ∏è Key,
üíπ Value,

usando tres matrices correspondientes. La representaci√≥n de salida de cada token 1Ô∏è‚É£ es calculada como una suma ponderada de los üíπ*values* de todos los tokens 2Ô∏è‚É£, donde el peso asignado a cada üíπ *value* (de los tokens 2Ô∏è‚É£) es calculado por una funci√≥n de compatibilidad entre su vector üóùÔ∏è *key* asociado (de los tokens 2Ô∏è‚É£) y el ‚ùì *query* del token cuya representaci√≥n est√° siendo calculada (token 1Ô∏è‚É£).


---
[^1]: Un **embedding** es una representaci√≥n vectorial densa de elementos discretos (como palabras, tokens o incluso √≠tems) en un espacio de dimensiones bajas. Al transformar elementos discretos en vectores, se pueden reflejar similitudes y relaciones entre ellos. Mientras que las representaciones discretas suelen ser muy dispersas, los embeddings permiten representar la informaci√≥n de forma compacta, lo que facilita el procesamiento por parte de las redes neuronales. En modelos como los Transformers, los embeddings se aprenden de forma conjunta con el resto de la arquitectura, permitiendo que la red adapte estas representaciones a la tarea espec√≠fica, ya sea traducci√≥n, an√°lisis de sentimientos, etc.